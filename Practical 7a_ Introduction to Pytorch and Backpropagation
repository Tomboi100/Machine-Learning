{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS0WKgSFEvff"
      },
      "source": [
        "## Deep learning framework - PyTorch\n",
        "\n",
        "PyTorch is a powerful and flexible framework that simplifies the process of building and training neural networks for deep learning. PyTorch's dynamic computation graphs and Pythonic API make it particularly well-suited for rapid prototyping and research.\n",
        "\n",
        "In this worksheet, we look into the details of how a deep learning framework operates, and implement  **gradient descent**, **backpropagation** using PyTorch, with its core APIs.\n",
        "\n",
        "- A multidimensional **Tensor** object, similar to numpy but with GPU acceleration.\n",
        "- An optimized **autograd** engine for automatically computing derivatives (for gradient descent algorithms).\n",
        "\n",
        "You can find more information about PyTorch by following one of the official tutorials or by reading the documentation.\n",
        "\n",
        "Colab has got Tensorflow and PyTorch installed already. It is recommended to run this notebooke using Colab (https://colab.research.google.com/), or on your own computer after proper libraries including PyTorch has been installed. In case you use Anaconda or miniconda and haven't got installed PyTorch yet, you can do it following the instruction here:\n",
        "\n",
        "https://pytorch.org/get-started/locally/\n",
        "\n",
        "e.g. to install PyTorch via Anaconda, use the following conda command::\n",
        "\n",
        "conda install pytorch torchvision -c pytorch\n",
        "\n",
        "or to install it in linux or MacOS via pip:\n",
        "\n",
        "pip3 install torch torchvision\n",
        "\n",
        "To enable the GPU based accelerated computing, you need to have proper GPUs like Nvidia GPUs, and install cuda https://docs.nvidia.com/cuda/\n",
        "\n",
        "If you have trouble installing PyTorch in your computer, or your computer does not have sufficient computational power, it would be easier to switch to Colab, or Kaggle kernel, or use some virtual machine in the cloud, where better computational resources (for RAM, CPU, GPU) are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF58WocPEvfw"
      },
      "outputs": [],
      "source": [
        "# Check if PyTorch is installed\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "\n",
        "# Check if you have suitable GPU and cuda installed for accelerated computing\n",
        "#  Running this worksheet does not require the use of GPU computing though.\n",
        "#\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLtmpVVVEvfx"
      },
      "source": [
        "# Tensors\n",
        "\n",
        "PyTorch _tensors_ supercharge NumPy arrays with GPU support and automatic differentiation, streamlining the deep learning development process.\n",
        "\n",
        " A Tensor is a multi-dimensional matrix containing elements of a single data type:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.empty(size): uninitiallized\n",
        "x = torch.empty(1) # scalar\n",
        "print(\"empty(1):\", x, \"size: \", x.size())\n",
        "x = torch.empty(3) # vector\n",
        "print(\"empty(3):\",x , \"size: \", x.size())\n",
        "x = torch.empty(2, 3) # matrix\n",
        "print(\"empty(2,3):\",x , \"size: \", x.size())\n",
        "x = torch.empty(2, 2, 3) # tensor, 3 dimensions\n",
        "#x = torch.empty(2,2,2,3) # tensor, 4 dimensions\n",
        "print(\"empty(2, 2, 3):\",x)\n",
        "\n",
        "# Initialise a tensor with random numbers\n",
        "# torch.randn(size): noramally distributed random numbers with mean 0 and sd 1\n",
        "x = torch.randn(3, 5)\n",
        "print(\"rand(3,5):\", x)\n",
        "\n",
        "# torch.rand(size): uniformly distributed random numbers within [0, 1]\n",
        "torch.rand(2, 3, 4)\n",
        "print(\"rand(2,3,4):\", x)\n",
        "\n",
        "# torch.zeros(size), fill with 0\n",
        "# torch.ones(size), fill with 1\n",
        "x = torch.zeros(5, 3)\n",
        "print(\"zeros(5,3):\", x)"
      ],
      "metadata": {
        "id": "qBRbqq8GutR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft2r89oVEvf3"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(3, 4, 2)\n",
        "\n",
        "# Check size\n",
        "print(\"size\", x.size())  # x.size(0)\n",
        "print(\"shape\", x.shape)  # x.shape[0]\n",
        "print(\"size: \", x.size())\n",
        "print(\"shape: \", x.shape) # same as x.size()\n",
        "print(\"x.size(2): \", x.size(2))\n",
        "print(\"x.shape[2]: \", x.shape[2]) # same as x.size(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Think about the datatype before printing\n",
        "# Check data type\n",
        "print(x.dtype)\n",
        "\n",
        "# specify types, float32 default\n",
        "x = torch.ones(2, 3, dtype=torch.float16)\n",
        "print(x)\n",
        "\n",
        "# check type\n",
        "print(x.dtype)\n",
        "\n",
        "# construct from data\n",
        "x = torch.tensor([5.5, 3])\n",
        "print(x, x.dtype)"
      ],
      "metadata": {
        "id": "DVbF3Kg2yAOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0_rywA2EvgT"
      },
      "source": [
        "Just like in numpy, we have element-wise sum, multiply, and other basic operations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Operations\n",
        "x = torch.ones(2, 2)\n",
        "y = torch.rand(2, 2)\n",
        "\n",
        "# elementwise addition\n",
        "z = x + y\n",
        "\n",
        "print(\"x:\", x)\n",
        "print(\"y:\", y)\n",
        "print(\"z:\", z)\n",
        "print(\"torch.add(x,y)\\n\", torch.add(x,y))\n",
        "print(\"y:\", y)\n",
        "\n",
        "# in place addition, everythin with a trailing underscore is an inplace operation\n",
        "# i.e. it will modify the variable\n",
        "y.add_(x)\n",
        "print(\"in place addition y.add_(x), y:\\n\", y)\n"
      ],
      "metadata": {
        "id": "eqW-4Ghty9CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaB2IAvYEvgU"
      },
      "outputs": [],
      "source": [
        "a, b = torch.ones(3, 4)*2, torch.ones(3, 4)*3\n",
        "c = torch.randn(4, 3)\n",
        "\n",
        "# addition, multiplication, etc are all element-wise\n",
        "summed = a + b\n",
        "mult   = a * b\n",
        "mult1 = torch.mul(a, b)\n",
        "power  = a ** 2\n",
        "sine   = torch.sin(a)\n",
        "\n",
        "print(\"a + b: \", summed)\n",
        "print(\"a*b: \", mult)\n",
        "print(\"torch.mul(a, b):\\n \", mult1)\n",
        "print(\"torch.sin(a): \\n\", sine)\n",
        "\n",
        "\n",
        "# subtraction\n",
        "c = a - b\n",
        "c = torch.sub(a, b)\n",
        "\n",
        "# division\n",
        "c = a/b\n",
        "c = torch.div(a,b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuynqBToEvgU"
      },
      "source": [
        "Question: What are the shapes of the tensors above (summed, mult, etc)? Print them to see if your intuition is correct."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix multiplication is done through torch.mm\n",
        "mult = torch.mm(a, c)\n",
        "print(\"torch.mm(a, c):\\n \", mmult)\n",
        "# Note that the following lines would both give an error. Why?\n",
        "# mult = a * c\n",
        "# torch.mm(a, b)\n",
        "# torch.mul(a,c)"
      ],
      "metadata": {
        "id": "COToPq_2zqOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKgWch9PEvgU"
      },
      "source": [
        "Indexing and slicing works as it does in numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdWYu76YEvgU"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(3,5)\n",
        "print(x)\n",
        "\n",
        "print()\n",
        "print('Element at 1st row and third column of x:', x[0, 2]) # note: zero-indexing\n",
        "print('        in first row of x:', x[0, :])\n",
        "print('     in first column of x:', x[:, 0])\n",
        "\n",
        "# You can also use basic slicing syntax; i:j refers to the range from i to j\n",
        "# (more precisely, i is the first element included, j is the first element\n",
        "#  excluded)\n",
        "print()\n",
        "print('middle two elements of each row of x:\\n', x[:, 1:3])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access individual elements or subsets in the tensor\n",
        "# Think about the results before print out the results\n",
        "#\n",
        "\n",
        "x = torch.randn(3, 2, 4)\n",
        "print(x)\n",
        "\n",
        "print(x[1])\n",
        "print(x[1, 0]) # same as x[1, 0, :] ??\n",
        "print(x[:, 0])\n",
        "\n",
        "# Get the actual value if only 1 element in your tensor\n",
        "print(x[1, 0, 3])\n",
        "print(\"x[1,0,3].item()\", x[1,0,3].item())"
      ],
      "metadata": {
        "id": "D9hywxcPh939"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0BA_pmmEvgV"
      },
      "source": [
        "## tensor.view()\n",
        "\n",
        "One of the most important skills in programming in Pytorch is reshaping a tensor. If you're not interested in the finer details of Pytorch, you can safely skip to the **backpropagation** section.\n",
        "\n",
        "Numbers are arranged in your tensor, but in memory, they are a single sequence of of numbers. That means for a matrix like\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "1 & 2 & 3\\\\\n",
        "40 & 50 & 60\\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "it will be stored in memory as\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "1 & 2 & 3 & 40 & 50 & 60 \\\\\n",
        "\\end{pmatrix}\\text{.}\n",
        "$$\n",
        "This is called row-major ordering.\n",
        "\n",
        "For higher order tensors the principle is the same: the memory layout scans first over the rightmost dimension. So, if we have a tensor $A$ with size ```(2, 2, 3)```, the elements are stored in the order:\n",
        "$$\n",
        "A_{111}, A_{112}, A_{113}, A_{121}, A_{122}, A_{123}, A_{211}, A_{212}, \\ldots\n",
        "$$\n",
        "Note that at every step the rightmost index increments first. When it gets to its maximum, the one to the left of it increments, and so on.\n",
        "\n",
        "### Reshaping\n",
        "\n",
        "We can take the data of one matrix in memory, and create a second matrix from it with another shape. This can done by the ```view()``` function in PyTorch, it takes a tensor and gives you a new _view_ on the same data, assuming a different shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKhaAq-vEvgW"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[3,5,2,6],[1, 3, 4,0],[-1,-3, -4,-0]])\n",
        "x = torch.tensor([[1,2,3,4],[5, 6, 7, 8],[-8,-7, -6,-5]])\n",
        "print(x)\n",
        "print(x.view(4, 3))\n",
        "print(x.view(2, 6))\n",
        "print(x.view(size=(12,)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmrebYxGEvgW"
      },
      "source": [
        "You can use ```-1``` for one of the arguments. Pytorch will work out what the size of that dimension is from the rest of the values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB_U7fOCEvgW"
      },
      "outputs": [],
      "source": [
        "a = torch.ones(3, 4)*2\n",
        "print(a)\n",
        "print(a.view(-1, 6))\n",
        "print(a.view(-1,))\n",
        "# print(a.view(-1, 3, -1)) # this doesn't work, why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmbmRdgwEvgX"
      },
      "source": [
        "Note the difference between matrix reshaping and the matrix transpose (done with the ```.t()``` function):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb6avYJAEvgX"
      },
      "outputs": [],
      "source": [
        "matrix = torch.tensor([[3,5,2],[1, 3, 4]])\n",
        "print(matrix.t())\n",
        "\n",
        "print(matrix.view(3, 2))\n",
        "\n",
        "y=matrix.view(3,2)\n",
        "y[0,0] = 10000\n",
        "print(matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUkUNoIuEvgY"
      },
      "source": [
        "```View()``` is nothing but an alternative way to interpret the original tensor without making a physical copy in the memory.  \n",
        "\n",
        "When you use ```reshape()``` instead of view, the matrix can accidentally copy a large matrix, not very efficient in use of memory, then later on the change you make on the reshaped matrix will not be reflected in the original matrix.  \n",
        "\n",
        "If you are not careful, you should use ```view()```, and check for errors.  For example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xYaFWQaEvgY"
      },
      "outputs": [],
      "source": [
        "print(matrix)\n",
        "y=matrix.t().reshape(2, 3) # this works, but copies the matrix without warning you\n",
        "y[0,0] = 99999\n",
        "print(matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWMI6XXUEvga"
      },
      "source": [
        "In short, make sure you understand the difference between _transposing_ (swapping dimensions) and view (changing the dimensions, but keeping the data the same)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhxSYLU8Evga"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "Let's look at how Pytorch implements _backpropagation_. All we need to do is create some tensors, tell Pytorch that we want to compute gradients for them, and then do some operations on them that result in a single scalar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a08m_pdNEvgc"
      },
      "outputs": [],
      "source": [
        "a, b = torch.randn(3, 4), torch.rand(3, 4)   # create some tensors ...\n",
        "a.requires_grad = True   # ... tell pytorch that we want gradients on a ...\n",
        "\n",
        "out = ((a + b) ** 2).sum()   # ... and perform a computation resulting in a scalar value.\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxmBmAc5Evgc"
      },
      "source": [
        "Pytorch has not just computed the result (```out```), it's also included a pointer to a ```SumBackward``` object representing the computation (sum) that created ```out```. This object links back to other objects, all the way down to the start of the computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsMcJWifEvgd"
      },
      "source": [
        "We've asked Pytorch to ensure we can compute a gradient on ```a``` and done some basic computation. The computation has resulted in a single number (```out```), so we can now compute the gradient of ```a``` over that output. Backpropagation only works efficiently if the output of the computation graph is a single scalar, usually your loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aP4wYn0Evgd"
      },
      "outputs": [],
      "source": [
        "print(a.grad)     # this is the gradient on a. Note it's currently empty\n",
        "\n",
        "out.backward()    # ask Pytorch to perform backprop\n",
        "\n",
        "print(a.grad)     # now a has a gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8BX9jk3Evgd"
      },
      "source": [
        "Note that the gradient of ```a``` has the same shape as ```a```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgdYuOO2Evge"
      },
      "source": [
        "## Learning\n",
        "\n",
        "Pytorch has many utilities to help you quickly build elaborate networks, but it's instructive to first see how you would use just these tools to build a simple model. As an example, we will build a simple **linear regression model**.\n",
        "\n",
        "In this worksheet, we will go through three different levels of implementation, get familiar with different unitility modules crucial for deep learning: .\n",
        "\n",
        "First, let's generate some simple random data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation 1"
      ],
      "metadata": {
        "id": "aOWtqWP5xMri"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkGWbLdmEvge"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(1000, 32)                      # 1000 instances, with 32 input features\n",
        "wt, bt = torch.ones(32, 1)*2, torch.ones(1)*1  # function to compute the true labels\n",
        "\n",
        "t = torch.mm(x, wt) + bt                       # the true target labels (ground truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: What is the true function for mapping from x to y? What are the weights?"
      ],
      "metadata": {
        "id": "BiXy1kAKbo8I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c02wwJQLEvge"
      },
      "source": [
        "Next up, we define the parameters of our model (we'll initialize them randomly)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBErCpnzEvge"
      },
      "outputs": [],
      "source": [
        "w = torch.randn(32, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRA9Wk8_Evgf"
      },
      "source": [
        "Note that any method that creates tensors (like ```torch.randn()```) can be told that it should make them require a gradient).\n",
        "\n",
        "Here's what one computation of the model output over the whole data looks like. We'll print the shapes of the tensors to see what's going on. **Before you run this cell, see if you can work out what the sizes will be.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "KWE5Zu5-Evgf"
      },
      "outputs": [],
      "source": [
        "print('          data size:', x.size())\n",
        "\n",
        "# model output\n",
        "y = torch.mm(x, w) + b\n",
        "\n",
        "print('        output size:', y.size())\n",
        "\n",
        "print()\n",
        "print('first 3 predictions:', y[:3, 0])\n",
        "print('       ground truth:', t[:3, 0]) # note that these will be completely different, because\n",
        "                                        # we haven't started training yet\n",
        "\n",
        "# residuals\n",
        "r = t - y\n",
        "print()\n",
        "print('     residuals size:', r.size())\n",
        "\n",
        "# mean-squared-error loss\n",
        "loss = (r ** 2).mean()\n",
        "print()\n",
        "print('               loss:', loss.item())\n",
        "# -- if you have a tensor with a single number, .item() will turn it into a normal float for you.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TPjz3j-Evgf"
      },
      "source": [
        "We can now apply backpropagation, and see that we get a gradient over our two parameters ```w``` and ```b```. Before you run the cell, what will the sizes of the gradient tensors be?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQGZn3MnEvgf"
      },
      "outputs": [],
      "source": [
        "loss.backward()\n",
        "\n",
        "print('gradient on w:', w.grad)\n",
        "print('gradient on b:', b.grad)\n",
        "\n",
        "# NB: if you run the cell twice, pytorch will complain. After each backward, pytorch expects a new forward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR726RZzEvgg"
      },
      "source": [
        "We are now ready to build a training loop. We'll use basic gradient descent without minibatches, computing the loss over the whole data every iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMznqcJ-Evgg"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "iterations = 21\n",
        "learning_rate= 0.5\n",
        "\n",
        "# regenerate the data and model\n",
        "x = torch.randn(1000, 32)                      # 1000 instances, with 32 features\n",
        "wt, bt = torch.randn(32, 1), torch.randn(1)    # parameters of the true model\n",
        "t = torch.mm(x, wt) + bt\n",
        "\n",
        "w = torch.randn(32, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "for i in range(iterations):\n",
        "\n",
        "    # forward pass\n",
        "    y = torch.mm(x, w) + b\n",
        "\n",
        "    # mean-squared-error loss\n",
        "    r = t - y\n",
        "    loss = (r ** 2).mean()\n",
        "\n",
        "    # backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # print the loss\n",
        "    print(f'iteration {i: 4}: loss {loss:.4}')\n",
        "\n",
        "    # weight update for gradient descent\n",
        "    w.data = w.data - learning_rate * w.grad.data\n",
        "    b.data = b.data - learning_rate * b.grad.data\n",
        "    # -- Note that we don't want the Pytorch's autodiff engine to compute gradients over this part.\n",
        "    #   by operrating on w.data, we are only changing the values of the tensor not\n",
        "    #   remembering a computation graph.\n",
        "\n",
        "    # delete the gradients\n",
        "    w.grad.data.zero_()\n",
        "    b.grad.data.zero_()\n",
        "    # -- if we don't do this, the gradients are remembered, and any new gradients are added\n",
        "    #    to the old.\n",
        "\n",
        "# show the true model, and the learned model\n",
        "print()\n",
        "print('true model: ', wt.data[:4].t(), bt.data)\n",
        "print('learned model:', w.data[:4].t(), b.data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation 2: Using predefined loss function and optimizers"
      ],
      "metadata": {
        "id": "1HhGbD7JxY1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function\n",
        "Instead of defining our own loss function as done in above, we can also use the built-in loss functions in PyTorch, e.g.\n",
        "\n",
        "- torch.nn.L1Loss\n",
        "- torch.nn.MSELoss\n",
        "- torch.nn.CrossEntropyLoss\n",
        "\n",
        "Typical use of loss function is:\n",
        "\n",
        "> criterion = torch.nn.MSELoss() # criterion is a function\n",
        "\n",
        "> loss = criterion(pred, target) # compute the loss\n",
        "\n",
        "> loss.backward() # later perform backward pass on loss\n"
      ],
      "metadata": {
        "id": "epZlC-bfrGAl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T2OdCbAEvgg"
      },
      "source": [
        "## torch.optim\n",
        "\n",
        "Now we'll first look at ```torch.optim```, which contains a number of _optimizers_. Using these, we don't have to implement the gradient descent step ourselves. This may not seem like a big part of our code, but it can get more complicated when we want to try variations on gradient descent like Adam. To illustrate, let's use the Adam optimizer in our linear regression example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6OiPMxkEvgh"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "# hyperparameters\n",
        "iterations = 101\n",
        "learning_rate= 2.0\n",
        "\n",
        "# regenerate the data and model\n",
        "x = torch.randn(1000, 32)                      # 1000 instances, with 32 features\n",
        "wt, bt = torch.randn(32, 1), torch.randn(1)    # parameters of the true model\n",
        "t = torch.mm(x, wt) + bt\n",
        "\n",
        "w = torch.randn(32, 1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "criterion = torch.nn.MSELoss() # criterion is a function\n",
        "\n",
        "# Create the optimizer. It needs to know two things:\n",
        "# - the learning rate\n",
        "# - which parameters its responsible for\n",
        "opt = Adam(lr=learning_rate, params=[w, b])\n",
        "\n",
        "for i in range(iterations):\n",
        "    # forward/backward, same as before\n",
        "    y = torch.mm(x, w) + b\n",
        "    # r = t - y\n",
        "    # loss = (r ** 2).mean() # Define a LOSS function\n",
        "    loss = criterion(t, y)   # Use predefined LOSS function\n",
        "\n",
        "    loss.backward()  # Perform backward pass on loss\n",
        "    # -- Note that the optimizer _doesn't_ compute the gradients.\n",
        "    #    The optimizer takes the gradients computed here, and uses them\n",
        "    #    to adapt the parameters.\n",
        "\n",
        "    # print the loss\n",
        "    if i % 20 == 0:\n",
        "        print(f'iteration {i: 4}: loss {loss:.4}')\n",
        "\n",
        "    #  Optimizer perform the gradient descent step to update weights\n",
        "    opt.step()\n",
        "\n",
        "    # Clear the gradients to zero\n",
        "    opt.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2KVaevcEvgh"
      },
      "source": [
        "You may have noticed that Adam (which is supposed to be better than gradient descent in many ways), actually takes longer to converge. That's because this is a very simple problem. Adam's strength shows when you train very large networks with many weights all doing different things."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation an MLP: Using torch.nn modules"
      ],
      "metadata": {
        "id": "aUZZ0eGLx6mu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjF6dZ9AEvgh"
      },
      "source": [
        "## torch.nn\n",
        "\n",
        "The package ```torch.nn``` contains utilities for building complex neural networks. It is built around <em>modules</em>: classes that combine a set of weights with a ```forward()``` function that uses these weights to compute a forward pass.\n",
        "\n",
        "The simplest module is probably ```torch.nn.Linear```. It implements a simple linear operation with a weight matrix and a bias vector (this is like the ```Dense``` layer in Keras)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvaUWaJjEvgh"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "lin = nn.Linear(2, 2) # a linear function from a 2-vector to a 2-vector\n",
        "\n",
        "print('weight matrix:', lin.weight)\n",
        "print()\n",
        "print('bias vector:', lin.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olqvEuB8Evgi"
      },
      "source": [
        "Pytorch knows that ```lin.weight``` and ```lin.bias``` are _parameters_. as they are ```nn.Parameter``` objects, a lightweight wrapper around the torch tensor, which signals that this tensor is meant to be treated like a model parameter. It has ```requires_grad=True``` by default, and there are helper functions to collect all parameters of a complex model.\n",
        "\n",
        "We can apply the linear transformation by calling ```lin``` just like a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YWBdu3HEvgi"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1.0, 2.0])\n",
        "lin(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iny0sJruEvgi"
      },
      "source": [
        "Note that the resulting tensor has a ```grad_fn``` attribute, so we can tell that the computation graph is being remembered.\n",
        "\n",
        "To implement a module of our own, we create a subclass of the ```nn.Module``` class. All we need to implement is the constructor and the ```forward``` function. Here is a module for a simple two-layer MLP with a ReLU activation on the hidden layer.\n",
        "\n",
        "To illustrate how to define and apply parameters, we will also add a multiplier to the output (a single learnable value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYBwIrypEvgi"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size = 16, hidden_size=32, out_size=1):\n",
        "        \"\"\"\n",
        "        This is the _constructor_ the function that creates an instance of the MLP class.\n",
        "\n",
        "        The argument 'self' is a standard argument in python object-oriented programming. It\n",
        "        refers to the current instance that we're creating. The other arguments are parameters\n",
        "        of the MLP.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # everything that has parameters should be created in the contructor\n",
        "        self.layer1 = nn.Linear(in_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, out_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # the layers have most of the parameters, but we will also add one of our own\n",
        "        self.mult = nn.Parameter(torch.tensor([1.0]))\n",
        "        # -- we create a tensor with the initial value, and wrap it in an nn.Parameter\n",
        "        #    objects. Because it's an nn.Parameter, pytorch will take care of the rest.\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        This is the function that gets executed when we call the module like a  function.\n",
        "\n",
        "        - The argument 'self' again refers to the current object.\n",
        "        - The argument 'x' is the input to the function (multiple arguments, named aguments\n",
        "          and even no arguments are possible)\n",
        "        \"\"\"\n",
        "\n",
        "        h = self.layer1(x)  # apply the first layer\n",
        "        h = self.relu(h)    # apply a ReLU activation\n",
        "        o = self.layer2(h)  # apply the second layer\n",
        "\n",
        "        o = o * self.mult        # apply the multiplier\n",
        "\n",
        "        return o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy93c5DCEvgj"
      },
      "source": [
        "We can now create an MLP instance, and feed it some data. Before you run the cell, can you predict the size of the output?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9c_sd79Evgj"
      },
      "outputs": [],
      "source": [
        "mlp = MLP()            # create an MLP with the standard dimensions\n",
        "\n",
        "x = torch.randn(3, 16) # three instances, with 16 features\n",
        "\n",
        "mlp(x)                 # pass the data through the MLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTtff1qGEvgj"
      },
      "source": [
        "Because we've subclassed ```nn.Module```, we get a lot of functionality for free. For instance, ```mlp``` has a function that lets us loop over all its parameters and the parameters of its modules (and the parameters of their modules and so on)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgoLTdkwEvgj"
      },
      "outputs": [],
      "source": [
        "for param in mlp.parameters():\n",
        "    print(param.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRhN2Y5lEvgk"
      },
      "source": [
        "In order, these are the multiplier, the weights matrix of the first layer, the bias of the first layer, the weight matrix of the second layer and the bias of the second layer.\n",
        "\n",
        "This is helpful when we need to let the optimizer know what the parameters of our model are.\n",
        "\n",
        "Here's an example of how to put everything together and train our MLP on some generated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsqgH1cEEvgk"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "iterations = 1000\n",
        "learning_rate= 0.01\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# regenerate the data and model\n",
        "x = torch.randn(1000, 32)                          # 1000 instances, with 2 features\n",
        "t = torch.sqrt(x.pow(2).sum(dim=1, keepdim=True))  # we'll use the vector norm as a target function\n",
        "\n",
        "model = MLP(32, 64, 1)\n",
        "\n",
        "opt = Adam(lr=learning_rate, params=model.parameters())\n",
        "# -- Note that we just point the optimizer to the parameters generator\n",
        "\n",
        "for i in range(iterations):\n",
        "\n",
        "    y = model(x)\n",
        "    loss = criterion(y, t)\n",
        "    # -- We'll switch to the pytorch implementation of the MSE.\n",
        "\n",
        "    if i % 50 == 0:\n",
        "        print(f'iteration {i: 4}: loss {loss:.4}')\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    opt.step()\n",
        "    opt.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiFPMbcTEvgp"
      },
      "source": [
        "\n",
        "### Further reading\n",
        "\n",
        "* Pytorch 60 minute blitz: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
        "* Learning Pytorch with examples: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
        "* Visualizing Models, Data, and Training with TensorBoard: https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NJxbYDHEvgp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}