{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC_My_TCs0Lz"
      },
      "source": [
        "# A first look at artificial neural networks\n",
        "\n",
        "## Artificial Neural Networks\n",
        "\n",
        "Artificial Neural Networks (ANNs) are computational processing systems that are heavily inspired by the way biological nervous systems function. The basic structure of an ANN can be visually conceptualised like:\n",
        "\n",
        "<a title=\"Glosser.ca, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg\"><img width=\"256\" alt=\"Colored neural network\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/256px-Colored_neural_network.svg.png\"></a>\n",
        "\n",
        "**Figure 1:** A simple three layered fully connected feedforward neural network (FNN), aka Multi-layer perceptrons (MLPs),comprisedof a input layer, a hidden layer and an output layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjhFVOzTs0L9"
      },
      "source": [
        "## Neural network modelling frameworks\n",
        "\n",
        "#### Tensorflow and Keras \n",
        "Keras (https://keras.io) is a highlevel and easy-to-use API built on top of Tensorflow (https://www.tensorflow.org) for developing and evaluating deep learning models. It was developed with a focus on enabling fast experimentation. \n",
        "\n",
        "In this worksheet, you will learn how to create a neural network using Tensorflow Keras API. \n",
        "\n",
        "Neural network models are trained by gradient descent. As we perform computations, Tensorflow memorises the computation graph that we build up. When it comes to compute the gradient, tensorflow can trace back over the computation graph (using the backpropagation algorithm) and work out all the required gradients (handled in TensorFlow via the [GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) object). \n",
        "\n",
        "When we build models with Keras, we don't usually have to worry about low level computation. High-level deep learning concepts translate to Keras APIs:\n",
        "\n",
        "- **Layers**, which are combined into a model A loss function, which defines the feedback signal used for learning\n",
        "- An **optimizer**, which determines how learning proceeds Metrics to evaluate model performance, such as accuracy\n",
        "- A training loop that performs **mini-batch** stochastic gradient descent\n",
        "\n",
        "Colab has got Tensorflow installed already. It is recommended to run this notebooke using Colab (https://colab.research.google.com/), or on your own computer after proper libraries including Tensorflow has been installed. In case you use Anaconda or miniconda and haven't got installed Tensorflow yet, you can do it following the instruction here:\n",
        "https://docs.anaconda.com/anaconda/user-guide/tasks/tensorflow/\n",
        "\n",
        "If you are not sure if you have got a suitable GPU, you can start with installing Tensorflow for CPU only.\n",
        "\n",
        "If you have trouble installing Tensorflow in your computer, or your computer does not have sufficient computational power, it would be easier to switch to Colab, or Kaggle kernel, or use some virtual machine in the cloud, where better computational resources (for RAM, CPU, GPU) are available. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4B06ZDAs0L9"
      },
      "source": [
        "# Import tensorflow, check version\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t27Ccl11IyIU"
      },
      "source": [
        "### Tensor\n",
        "In general, all current machine-learning systems use tensors as their basic data structure — usually for numerical data. \n",
        "\n",
        "Tensors are a generalization of matrices to an arbitrary number of dimensions (note that a dimension is often called an axis in a tensor). NumPy arrays can be called tensors.\n",
        "\n",
        "* Scalar (rank-0 tensor).\n",
        "* Vectors (rank-1 tensor).\n",
        "* Matrices: rank-2 tensors.\n",
        "* Rank-3 tensors and higher-rank tensors (e.g. array x below are Rank-3 tensors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMuSjBg2LO7m"
      },
      "source": [
        "import numpy as np\n",
        "x = np.array([[[5, 78, 2, 34, 0],\n",
        "                   [6, 79, 3, 35, 1],\n",
        "                   [7, 80, 4, 36, 2]],\n",
        "                  [[5, 78, 2, 34, 0],\n",
        "                   [6, 79, 3, 35, 1],\n",
        "                   [7, 80, 4, 36, 2]],\n",
        "                  [[5, 78, 2, 34, 0],\n",
        "                   [6, 79, 3, 35, 1],\n",
        "                   [7, 80, 4, 36, 2]]])\n",
        "print(x.ndim)\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constant tensors (Not assignable!!)\n",
        "# All ones/zero tensors\n",
        "x = tf.ones(shape=(2, 1))\n",
        "print(x)\n",
        "x = tf.zeros(shape=(2, 1))\n",
        "print(x)\n",
        "\n",
        "# Random tensors\n",
        "x = tf.random.normal(shape=(3, 1), mean=0., stddev=1.) \n",
        "print(x)\n",
        "\n",
        "x = tf.random.uniform(shape=(3, 1), minval=-1., maxval=1.) \n",
        "print(x)\n",
        "\n",
        "x = 0 # Scalar\n",
        "print(x)"
      ],
      "metadata": {
        "id": "6n9R6Hhnho6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unlike numpy array, tensor constant is not assignable.\n",
        "# see e.g. error raised when trying to assign a value to tensor constant\n",
        "import numpy as np\n",
        "x = np.ones(shape=(2, 2))\n",
        "x[0, 0] = 0.\n",
        "\n",
        "x = tf.ones(shape=(2,2))\n",
        "x[0, 0] = 0."
      ],
      "metadata": {
        "id": "mMmqo54dhsDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To modify tensors, tensor Variabble is used in tensorflow\n",
        "v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))\n",
        "print(v)\n",
        "\n",
        "# Assign values to a tensor variable\n",
        "v.assign(tf.ones((3, 1)))\n",
        "print(v)\n",
        "\n",
        "# Modify element with index [0,0] in tensor variable v to 0\n",
        "# Your code here\n",
        "#"
      ],
      "metadata": {
        "id": "vkaaoC0Ohswi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gO-76xkZhvrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90Jl1oeVs0L-"
      },
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDewRWb3s0L-"
      },
      "source": [
        " The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. The MNIST data-set is about 12 MB and will be downloaded automatically if it is not located in the given path."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd9z5PCXs0L_"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(image_train, label_train), (image_test, label_test) = mnist.load_data()\n",
        "\n",
        "print(\"Size of:\")\n",
        "print(\"- Training-set:\\t\\t{}\".format(image_train.shape))\n",
        "print(\"- Test-set:\\t\\t{}\".format(image_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX9dkWEhzuGG"
      },
      "source": [
        "# Your code: \n",
        "# print label_train, print its shape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmjT6wLWs0L_"
      },
      "source": [
        "The data-set is split into 3 mutually exclusive sub-sets. We will only use the training and test-sets in this tutorial.\n",
        "\n",
        "Define a simple function to have a look at the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v9QakdAs0MA"
      },
      "source": [
        "## Getting to know our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J3w9Otps0MA"
      },
      "source": [
        "The following method is plots 9 images from the dataset in a 3x3 grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cayq2d-hs0MA"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_images(images, cls_true, cls_pred=None):\n",
        "    assert len(images) == len(cls_true) == 9\n",
        "    \n",
        "    # Create figure with 3x3 sub-plots.\n",
        "    fig, axes = plt.subplots(3, 3)\n",
        "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # Plot image.\n",
        "        #ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
        "        ax.imshow(images[i], cmap='binary')\n",
        "\n",
        "        # Show true and predicted classes.\n",
        "        if cls_pred is None:\n",
        "            xlabel = \"True: {0}\".format(cls_true[i])\n",
        "        else:\n",
        "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
        "\n",
        "        ax.set_xlabel(xlabel)\n",
        "        \n",
        "        # Remove ticks from the plot.\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        \n",
        "    # Ensure the plot is shown correctly with multiple plots\n",
        "    # in a single Notebook cell.\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXNBketEs0MB"
      },
      "source": [
        "Plot a few images to see if data is correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeRYW7las0MC"
      },
      "source": [
        "# We know that MNIST images are 28 pixels in each dimension.\n",
        "img_size = 28\n",
        "\n",
        "# Images are stored in one-dimensional arrays of this length. (might want to use this later for input_size...)\n",
        "img_size_flat = img_size * img_size\n",
        "\n",
        "# Tuple with height and width of images used to reshape arrays.\n",
        "img_shape = (img_size, img_size)\n",
        "\n",
        "# Number of classes, one class for each of 10 digits.\n",
        "num_classes = 10\n",
        "\n",
        "# Get the first images from the test-set.\n",
        "images = image_train[0:9]\n",
        "\n",
        "# Get the true classes for those images.\n",
        "cls_true = label_train[0:9]\n",
        "\n",
        "# Plot the images and labels using our helper-function above.\n",
        "plot_images(images=images, cls_true=cls_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN4Zjheps0MC"
      },
      "source": [
        "For this first network, we'll flatten everything into a 784-dimensional feature vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHohqH6GbAnS"
      },
      "source": [
        "# In general, you may select between any two indices along each tensor axis. \n",
        "# For instance, in order to select 14 × 14 pixels in the bottom-right corner for image with index 10, you do this:\n",
        "my_slice = image_train[10, 14:, 14:]\n",
        "print(my_slice.shape)\n",
        "my_slice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhGG9IsneAcM"
      },
      "source": [
        "The images we get were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. We transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSQnMZ7Js0MD"
      },
      "source": [
        "# Prepare data, flatten the input and rescale\n",
        "X_train = image_train.reshape((60000, 28*28)) # or #X_train = image_train.reshape(X_train.shape[0], -1)\n",
        "X_train = X_train.astype('float32')/255 \n",
        "X_test = image_test.reshape((10000, 28*28)) # or #X_test = image_test.reshape(X_test.shape[0], -1)\n",
        "X_test = X_test.astype('float32')/255\n",
        "\n",
        "print(\"Size of:\")\n",
        "print(\"- Training-set:\\t\\t{}\".format(X_train.shape))\n",
        "print(\"- Test-set:\\t\\t{}\".format(X_test.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaNQwQt9s0MD"
      },
      "source": [
        "#### To build a fully connected feedforward network (aka Multilayer Perceptron (MLP))\n",
        "The first model we'll use is a simple fully connected feedforward network. This is called a Dense layer in Keras. -\n",
        "#### Dimensionality reduction with PCA\n",
        "Since fully connected layers are a bit heavy on image data (and you're probably running this on your laptop), we'll reduce the dimensionality of the data by [PCA (principal component analysis)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). PCA decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. We will discuss topic on dimensionalilty reduction in a few weeks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyyCAPaUs0MD"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=60) # reduce to 60 dimensions\n",
        "pca.fit(X_train)\n",
        "\n",
        "X_train = pca.transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "print(X_train.shape) \n",
        "print(X_test.shape) # NB: none of this is Keras yet. We're just using sklearn on some numpy arrays"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTAMootds0ME"
      },
      "source": [
        "The training labels are encoded as integers. We need these as one-hot vectors instead, so we can match them to the ten outputs of the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_nB-FCns0ME"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "print(label_train.shape, label_test.shape)\n",
        "\n",
        "y_train = to_categorical(label_train)\n",
        "y_test = to_categorical(label_test)\n",
        "\n",
        "print(y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMyFcP_OxUrd"
      },
      "source": [
        "### Question: \n",
        "Look at the one-hot coding given by y_train[0], which digit does it represent?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scpTWCx6s0ME"
      },
      "source": [
        "print(y_train[0]) # print the one-hot vector for the first example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjTdngnUs0ME"
      },
      "source": [
        "## Keras Model\n",
        "\n",
        "The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the ```Sequential``` model, a linear stack of **layers**. Before we start take some time to read the ```Sequential``` documentation:\n",
        "\n",
        "https://keras.io/getting-started/sequential-model-guide/\n",
        "\n",
        "Note: Keras has two APIs for this: the **Sequential** API and the **Model** API. The sequential API (the simplest) assumes that your model is a simple sequence of operations, usually neural network layers. The input is passed through the first layer, the result of that is passed through the second and so on. \n",
        "\n",
        "This is useful for simple NN models where you are only interested in the input and output. If your model gets more complex, you may want to use the Model API (we'll use in the next practicals)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, creating a Sequential model. \n",
        "\n",
        "We can create a Sequential model incrementally via the add() method."
      ],
      "metadata": {
        "id": "XtoobyAIibrs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGh7bopps0MF"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "model = Sequential() # or specify model name: model = Sequential(name=\"my_first_model\")\n",
        "model.add(keras.Input(60)) # Input layer, input shape setup here should match with the input data when fitting or making prediction.\n",
        "model.add(layers.Dense(32, activation='relu'))#, input_shape=(60,))) # first dense layer, 32 hidden units,\n",
        "model.add(layers.Dense(10, activation='softmax'))# second dense layer, output class probabilities with softmax activation\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check model weights\n",
        "model.weights"
      ],
      "metadata": {
        "id": "KrDASu5JMZ4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also create a Sequential model by passing a list of layers to the Sequential constructor:"
      ],
      "metadata": {
        "id": "euz9tRwQjCfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential(    \n",
        "    [\n",
        "     keras.Input(shape=(60)),   \n",
        "     layers.Dense(32, activation='relu'),\n",
        "     layers.Dense(10, activation='softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Get the list of layers in the model\n",
        "model.layers"
      ],
      "metadata": {
        "id": "AaBwBcsUjDv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: \n",
        "1. how many hidden layers does this model have?\n",
        "\n",
        "2. how to compute the total number of parameters or weights? \n",
        "(Remeber to add the parameters for bias)\n",
        "\n",
        "\n",
        "Once create the model, you can start training and validating the model. https://keras.io/guides/training_with_built_in_methods/"
      ],
      "metadata": {
        "id": "1uSOhe10jqr0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEl2wlGss0MF"
      },
      "source": [
        "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n",
        "#optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5_pmbSms0MG"
      },
      "source": [
        "## Training\n",
        "\n",
        "We've also told the compiler that we'd like it to compute accuracy for us during training (since categorical cross-entropy loss is a bit hard to interpret).\n",
        "\n",
        "We're now ready to start training using the fit() method:\n",
        "- The **batch size** to use within each epoch of mini-batch gradient descent: the number of training examples considered to compute the gradients for one weight update step.\n",
        "- The number of **epochs** to train for: how many times the training loop should iterate over the data passed. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LrbqJ2C8bt18"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCLOxVQ4s0MH"
      },
      "source": [
        "# Train the model, iterating on the data in batches of 32 samples\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=1000);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validating##\n",
        "A deep learning model should never be evaluated on its training data alone - it’s standard practice to use a validation set to monitor the accuracy (or other performance metric) of the model during training. Here, we randomly set apart 10% of training samples from the original training data for validation using validation_split argument in fit() method. Usually, we create a validation set separately, and set \"validation_data\" in fit(). For more details on keras model training api, check https://keras.io/api/models/model_training_apis/"
      ],
      "metadata": {
        "id": "Cj4fU6J0gNz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The call to fit() returns a **History** object. This object contains a history field, which is a dict mapping keys such as \"loss\" or specific metric names to the list of their per-epoch values."
      ],
      "metadata": {
        "id": "-cphMmVcc4P-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_split=0.1, epochs=5, batch_size=1000);"
      ],
      "metadata": {
        "id": "WHS9LVFrhPxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consider increase the number of epochs if the model loss or traininng performance is not good enough or not converged\n",
        "# "
      ],
      "metadata": {
        "id": "5JfQdrvJkDFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "id": "HFtkwMFnmOBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "id": "ikp6yjC6c6Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the training and validation loss\n",
        "import matplotlib.pyplot as plt\n",
        "history_dict = history.history\n",
        "loss_values = history_dict[\"loss\"]\n",
        "val_loss_values = history_dict[\"val_loss\"]\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OQiTm4UOkcUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the training and validation accuracy\n",
        "# Your code here:\n",
        "# \n"
      ],
      "metadata": {
        "id": "4viP9LZTm2NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After the model training is complete\n",
        "# evaluate the model \n",
        "results = model.evaluate(X_test, y_test)\n",
        "print(\"test loss, test acc:\", results)\n",
        "\n",
        "#_, accuracy = model.evaluate(X_test, y_test)\n",
        "#print('Accuracy: %.2f' % (accuracy*100))"
      ],
      "metadata": {
        "id": "UWqDHBapkRxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inference##\n",
        "To make predictions using a trained model "
      ],
      "metadata": {
        "id": "3WgFgXDgd__W"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwaE0ysss0MH"
      },
      "source": [
        "# To make predictions and get the confusion matrix\n",
        "# model.predict will generate probability for all 10 digits, \n",
        "#    so we use argmax to pick the class with the highest probability\n",
        "from sklearn import metrics\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred[0]) # print the prediction for first image\n",
        "\n",
        "print('\\nConfusion matrix: ')\n",
        "matrix = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(matrix)\n",
        "\n",
        "print('\\nClassification report: ')\n",
        "print(metrics.classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBdSC5gDs0MI"
      },
      "source": [
        "### Tasks\n",
        "\n",
        "As you now have a model to play with (in no particular order):\n",
        "\n",
        "1. Change the `learning-rate` for the optimiser.\n",
        "2. Change the ```batch_size``` to e.g. 1 or 1000 (and see how learning rate deals with larger/smaller batch sizes).\n",
        "3. Change the optimiser: from SGD to Adam\n",
        "4. Add complexity to the model, being mindful of how \"powerful\" your computers are.\n",
        "\n",
        "7. Try and find a sweet-spot between the size and performance of the model, take into account things like the number of iterations/epochs required to train the model in these assumptions.\n",
        "\n",
        "6. Do you get the exact same results if you run the Notebook multiple times without changing any parameters? Why or why not?\n",
        "\n",
        "5. Do you think these changes will have the same effect (if any) on other classifiers?\n",
        "\n",
        "8. Investigate **effect of dimensionality reduction** (with PCA, e.g. you may try to build the model without PCA and compare ...)\n",
        "\n",
        "9. Try out some **other ML classifiers** that have been discussed in the class, e.g. kNN (with scikit-learn) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "La9Ah0Qms0MI"
      },
      "source": [
        "# Your code\n",
        "#\n",
        "#\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}