{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cd07b084-2832-146b-f520-298c445446fe",
        "_uuid": "6c576d512dd0068b18ce361559b3d35d66808e84",
        "id": "vtKa3UcU7q7W"
      },
      "source": [
        "# Text classification with LR, NB and SVM\n",
        "\n",
        "Document/Text classification is one of the important and typical task in supervised machine learning (ML), with many applications such as spam filtering, email routing, sentiment analysis.\n",
        "\n",
        "In this worksheet we will go through a text based sentiment analysis using some linear and nonlinear models. We will learn how to extract features from text.\n",
        "\n",
        "We also peform grid search and randomized search for tuning the hyperparameters for support vector machines. Randomised search is often more efficient (requires much less run time) than grid search when it comes to hyperparameter tuning search, see [the comparison](https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html)\n",
        "\n",
        "\n",
        "\n",
        "**Be aware that** It might take too long and too much memory to train and tune the hyperparameters for standard SVMs, if working on local machines (in B57) given the large sample size for this dataset. Note that, a kernel matrix of size n*n is to be constructued for SVM training (n is the number of training samples). On the other hand, SVMs would be quite useful to work with problems with n samples and d variables and n << d\n",
        "\n",
        "Here's the documentation for the SVM in sklearn:\n",
        "- https://scikit-learn.org/stable/modules/svm.html\n",
        "\n",
        "Please familarise yourself with the idea of examining documentation without prompt as you need to get used to it for your assignment and future data science work.\n",
        "\n",
        "This worksheet was adapted from Eugen Anghel's [python notebook](https://www.kaggle.com/eugen1701/predicting-sentiment-and-helpfulness/data)\n",
        "\n",
        "# Sentiment analysis using Amazon review dataset\n",
        "\n",
        "We will be reviewing a Kaggle featured dataset of over 500k Amazon customer food reviews, provided by Stanford Network Analysis Project. Our aim is to develop models for predicting whether a customer review message is positive or negative. We will not focus on the Score, but only the positive/negative sentiment of the recommendation.\n",
        "\n",
        "We will go through several key steps for this machine learning task:\n",
        "\n",
        "- Extracting features from text files, such as **bag of words** and **Term Frequency - Inverse Document Frequency** (TF-IDF).\n",
        "- Splitting data into **training/validation/testing**\n",
        "- Fitting Naive Bayes, logistic regression, and SVM models\n",
        "- Evaluating model performance using **ROC analysis**\n",
        "- Selecting hyperparameters and models using Scikit-learn pipeline and **grid search** based on validation set\n",
        "- Making prediction on the test set, and evaluate\n",
        "- Examining important features for the prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a44628c0-7150-1d49-6904-79815f3342cb",
        "_uuid": "73fb46ae92581b0135e22ff5c0f95786b2c2c777",
        "id": "mlRWcY3f7q7f"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWsrR5n87q7f"
      },
      "source": [
        "\n",
        "## Loading the data\n",
        "We will only use a subset of the Amonzon review data with 10k comments for this exercise. For more detailed description of the data, please see:\n",
        "http://snap.stanford.edu/data/web-FineFoods.html\n",
        "\n",
        "As we only want to get the global sentiment of the review (positive or negative), we will purposefully ignore all Scores equal to 3. If the score id above 3, then the recommendation wil be set to \"postive\". Otherwise, it will be set to \"negative\".\n",
        "\n",
        "The data will be split into an training set and a test set with a test set ratio of (e.g.) 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5ae809b6-9c27-4b91-b626-e044aaa96153",
        "_uuid": "2b8fc6a9aae63692a5badaaa06a60e759ea152b8",
        "id": "ng7SEXjO7q7g"
      },
      "source": [
        "Let's first check whether we have the dataset available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwcl8DPQ_znA"
      },
      "outputs": [],
      "source": [
        "# For google colab user only. You can mount your google drive, and use the folder\n",
        "# as in any virtual machine.\n",
        "# You can also explore your good drive by clicking [folder icon] in the menu\n",
        "#  to left of the colab notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO2iKUDfD9_G"
      },
      "outputs": [],
      "source": [
        "# Change the directory appropriate to you\n",
        "dir = '/content/gdrive/MyDrive/CSM6420-2024/data'\n",
        "messages = pd.read_csv(os.path.join(dir, \"review10k.csv\"), sep=\",\")\n",
        "\n",
        "messages.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38SVgxnj7q7h"
      },
      "outputs": [],
      "source": [
        "# Only pick columns of interest, and rename a few columns\n",
        "dir = \"\"\n",
        "messages = pd.read_csv(os.path.join(dir, \"review10k.csv\"), sep=\",\")\n",
        "\n",
        "messages = messages[[\"Score\", \"Summary\",\"HelpfulnessNumerator\",\"HelpfulnessDenominator\"]]\n",
        "messages = messages.rename(columns={\"HelpfulnessNumerator\": \"VotesHelpful\", \"HelpfulnessDenominator\": \"VotesTotal\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1fp8mYp7q7h"
      },
      "outputs": [],
      "source": [
        "messages.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqzabQw37q7i"
      },
      "outputs": [],
      "source": [
        "# Your code here:\n",
        "# Check the distribution of the score (histogram)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yOs94wd7q7i"
      },
      "outputs": [],
      "source": [
        "# You code here:\n",
        "# Check the distribution of VotesHelpful.\n",
        "# Hint: Some data transformation (e.g. by taking the log) might be needed\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7a4b0df9-7054-34da-97eb-19035f74690e",
        "_uuid": "41b364e5d2191a3afc8900bd226af0b1a57bd740",
        "id": "m0n7hoei7q7i"
      },
      "source": [
        "Now let's select only what's of interest to us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e27b0181-a261-55c1-014a-4c56389ccf86",
        "_uuid": "437b18f34b556be74cbc720cd52c89b8b1fc6d7e",
        "id": "3VwNxR5x7q7i"
      },
      "outputs": [],
      "source": [
        "messages = messages[messages.Score!=3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ad6bcde8-eb5c-1c16-74ed-aea12d93cf1e",
        "_uuid": "ed5202b6bcde930fc197633e405bf00c31cb85d3",
        "id": "ciPFi0G17q7j"
      },
      "source": [
        "Let's see what we've got:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c7b9ff1d-9a1e-7703-c431-942a89839760",
        "_uuid": "e29150ef5b7036495addba210c54ade83d12a2bf",
        "id": "T0e6J0Zh7q7j"
      },
      "outputs": [],
      "source": [
        "messages.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "26e2dea6-34db-4fbb-eee4-9a4ad204f750",
        "_uuid": "33c87d80d9fc627fb9c1daae9335bb4321735233",
        "id": "kK5EAPre7q7j"
      },
      "source": [
        "Let's add the **Sentiment** column that turns the numeric score into either *positive*=1 or *negative* =0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5079ffb3-8622-a1a2-f308-06d3ebbf745b",
        "_uuid": "db43550d03a1c3a3e570906b24b24fc92113e65f",
        "id": "_VknzQqN7q7j"
      },
      "outputs": [],
      "source": [
        "messages[\"Sentiment\"] = messages[\"Score\"].apply(lambda x: 1 if x > 3 else 0)\n",
        "messages[\"Sentiment\"] = messages[\"Sentiment\"]\n",
        "messages.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OyHXnON7q7j"
      },
      "outputs": [],
      "source": [
        "# Your code here:\n",
        "# Check the distribution or correlation of Sentiment and Usefulness (VotesHelpful)\n",
        "# to answer the question: will you consider adding \"VotesHelful\" for predicting Sentiment?\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bc3de956-fba5-abc8-feb1-c857ebf0e17b",
        "_uuid": "984acfd0eb583434680e92befc6b57048b08b56f",
        "id": "k-IaReM87q7k"
      },
      "source": [
        "Now, try to look at some potential words linking to sentiment.\n",
        "Let's have a look at some 5s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8e10ced5-0ae6-69bd-5b9f-ccad8062f639",
        "_uuid": "46edde349b4fc6130016a67ff7e0a97d3ac90241",
        "id": "SmFXStBW7q7k"
      },
      "outputs": [],
      "source": [
        "messages[messages.Score == 5].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5321ff57-addb-483f-fc19-ad5b404780f0",
        "_uuid": "c421f1d031fa9531eb961bcbcc8521d9d739dac5",
        "id": "tgDBME667q7k"
      },
      "source": [
        "And some 1s as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c6606333-2c60-4e05-bb26-851cd20d2e58",
        "_uuid": "c1f61d8c998f57c55a6a28bc533b70dcd1e72e2b",
        "id": "XTNvc9Yn7q7k"
      },
      "outputs": [],
      "source": [
        "messages[messages.Score == 1].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "48eghaEwPlvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22hzl_cjQH-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d305041a-8445-1b8d-dd65-2275032e6655",
        "_uuid": "e2f71f384f44ffbc55450892ea958d19a9601c3f",
        "id": "TiNAxXyg7q7k"
      },
      "source": [
        "## Extracting features from text data\n",
        "\n",
        "Text data are usually series of ordered words, which need to be converted to numerical features before feeding to machine learning algorithms.\n",
        "\n",
        "- **Bag of words**: Bag of words model is one popular representation for text, in which we segment each text file (one English review mesage separated by space in this case) into words, and count # of times each word occurs in each document and finally assign each word an integer id. Each unique word in our dictionary will correspond to a feature (descriptive feature).\n",
        "\n",
        "- **TF** (Term Frequency): Just counting the number of words in each document has 1 issue: it will give more weights to longer documents than shorter documents. To avoid this, we can use frequency, i.e. TF = #count of word / #Total words, in each document.\n",
        "\n",
        "- **TF-IDF** (Term Frequency times Inverse Document Frequency): Finally, we can even reduce the weight of more common words like (the, is, an etc.) which occurs in most document. This is called TF-IDF i.e Term Frequency times inverse document frequency, where inverse document frequency = log(#Total docs / #count of doc containing word). See: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
        "\n",
        "Scikit-learn has a high level component which will create such feature vectors for us ‘CountVectorizer’ and 'TfidfTransformer'.\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "See more here (perhaps after class): http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "63c67fe7-61ed-357a-1be4-ede729c432e2",
        "_uuid": "ace2a3a107c5dfd930740d7cacaf8d2e56879d1e",
        "id": "fdkcYVcc7q7l"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "import re #regular expression module\n",
        "import string\n",
        "#import nltk # natural languaage toolkit (https://www.nltk.org/)\n",
        "\n",
        "cleanup_re = re.compile('[^a-z]+') #string that contains lowcase alphabets only\n",
        "def cleanup(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = cleanup_re.sub(' ', sentence).strip()\n",
        "    #sentence = \" \".join(nltk.word_tokenize(sentence)) # No need to do tokenisation here as Scikit-Learn can do it for you in CountVectorizeer)\n",
        "    return sentence\n",
        "\n",
        "messages[\"Summary_Clean\"] = messages[\"Summary\"].apply(cleanup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iigukcpG7q7l"
      },
      "outputs": [],
      "source": [
        "# Your code here:\n",
        "# Check the first 10 records of Summary and Summary_Clean (to understand the effect of cleanup)\n",
        "messages[[\"Summary\",\"Summary_Clean\"]][0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiA-14nR7q7l"
      },
      "outputs": [],
      "source": [
        "# Getting the indices for splitting data into training, and test set\n",
        "#train, test = train_test_split(messages, test_size=0.2)\n",
        "# Set random seed to ensure result reproducibility\n",
        "# Here train, test are the indices for the examples\n",
        "random_seed = 12345\n",
        "train, test = train_test_split(np.arange(messages.shape[0]), test_size=0.2, random_state= random_seed)\n",
        "print(\"%d items in training set, %d in test set\" % (len(train), len(test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "770a11cc-d711-6bc5-dc20-e41cde133f78",
        "_uuid": "7c14350f79fc9dc63ae3040ddadb522b24741741",
        "id": "Hub8ZYed7q7l"
      },
      "outputs": [],
      "source": [
        "count_vect = CountVectorizer(min_df = 1, ngram_range = (1, 4))\n",
        "X_train_counts = count_vect.fit_transform(messages.iloc[train][\"Summary_Clean\"])\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "\n",
        "X_test_counts = count_vect.transform(messages.iloc[test][\"Summary_Clean\"])\n",
        "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
        "\n",
        "y_train = messages.iloc[train][\"Sentiment\"]\n",
        "y_test = messages.iloc[test][\"Sentiment\"]\n",
        "\n",
        "\n",
        "# Initialise the dictionary to store prediction results\n",
        "prediction = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SinWyZHq7q7l"
      },
      "outputs": [],
      "source": [
        "# Checking dataset dimensionality\n",
        "print(X_train_counts.shape)\n",
        "print(X_train_tfidf.shape)\n",
        "print(X_test_counts.shape)\n",
        "print(X_test_tfidf.shape)\n",
        "\n",
        "# Get the indices of nonzero elements in the sparse matrix\n",
        "rows, cols = X_train_tfidf.nonzero()\n",
        "# Print the first 5 nonzero elements\n",
        "print(X_train_tfidf[rows[0:5], cols[0:5]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5d98557f-721e-370f-f050-4b6ae2d54634",
        "_uuid": "5b29c6566b6dbb027e06702a36436bb87b246a68",
        "id": "JxkmGyZe7q7m"
      },
      "source": [
        "## Let's get fancy with WordClouds!\n",
        "\n",
        "Note: just in case of missing library 'worcloud' , run the following to install it. Or otherwise you don't have to run it yourself, just look at the existing figures.\n",
        "\n",
        "$ conda install -c conda-forge wordcloud\n",
        "\n",
        "or install wordcloud uisng pip within the notebook\n",
        "\n",
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtW2CJewBGmE"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "# Check what are the listed stopwords??\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "71916e96-38e7-0480-54f2-d4bce47eb543",
        "_uuid": "5f5c79ebaab98a8ae2c9b1eb2a2b6864efabc9fb",
        "id": "79iscFWn7q7m"
      },
      "outputs": [],
      "source": [
        "mpl.rcParams['font.size']=12                #10\n",
        "mpl.rcParams['savefig.dpi']=100             #72\n",
        "mpl.rcParams['figure.subplot.bottom']=.1\n",
        "\n",
        "def show_wordcloud(data, title = None):\n",
        "    wordcloud = WordCloud(\n",
        "        background_color='white',\n",
        "        stopwords=stopwords,\n",
        "        max_words=200,\n",
        "        max_font_size=40,\n",
        "        scale=3,\n",
        "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
        "    ).generate(str(data))\n",
        "\n",
        "    fig = plt.figure(1, figsize=(8, 8))\n",
        "    plt.axis('off')\n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=20)\n",
        "        fig.subplots_adjust(top=2.3)\n",
        "\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.show()\n",
        "\n",
        "show_wordcloud(\" \".join(messages[\"Summary_Clean\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1g2l18JBGmF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f576e1d9-7423-24e1-1009-a6db0166ecfb",
        "_uuid": "1633db5ed147bb935272737fe568048090e8d54e",
        "id": "C-_RpNrr7q7n"
      },
      "source": [
        "We can also view wordclouds for only positive or only negative entries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3b036261-1732-bd7f-34f1-35994827bce0",
        "_uuid": "455178ca59abc7190ac1e505bffc584ff74f28d8",
        "id": "HuJK_hQ67q7o"
      },
      "outputs": [],
      "source": [
        "show_wordcloud(\" \".join(messages[messages.Score == 1][\"Summary_Clean\"]), title = \"Low scoring\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6c43d96d-f9e3-9f2d-eaf8-fcd399e37968",
        "_uuid": "390cc9e09f7e46685e215f7be989770e7013ecc9",
        "id": "TYS20zIW7q7o"
      },
      "outputs": [],
      "source": [
        "show_wordcloud(\" \".join(messages[messages.Score == 5][\"Summary_Clean\"]), title = \"High scoring\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6eb57738-2179-65e2-31f6-5f9671a17a18",
        "_uuid": "ae0d879f46a912a5fdac7f709708b99168387914",
        "id": "keN6wc097q7o"
      },
      "source": [
        "## Applying Multinomial Naïve Bayes\n",
        "\n",
        "Note: to computer AUC, we need to return a continous score or a probability output using e.g. predict_proba, which will return often multiple columnns for different classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ed4eeb1c-0b9e-838b-c0f6-cbc781701fc2",
        "_uuid": "abbb571f2639282323895e9a07c7026faa1a1ec5",
        "id": "SNYVmAx87q7o"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb = MultinomialNB().fit(X_train_tfidf, y_train)\n",
        "\n",
        "prediction['Multinomial'] = nb.predict_proba(X_test_tfidf)[:,1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3e411b7a-8889-ed9e-2851-7c6e977642f9",
        "_uuid": "4933c2c69d1faa39383d7b00922d34a440409971",
        "id": "I9G1n2UB7q7p"
      },
      "source": [
        "## Applying Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "041f7e6a-e3da-89bb-5cf7-44230f591282",
        "_uuid": "e6ca8b49d1b80508a5149b748dc039b71b06e885",
        "id": "E1PqKSiI7q7p"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()#(C=1e3)\n",
        "lr=logreg.fit(X_train_tfidf, y_train)\n",
        "prediction['Logistic'] = logreg.predict_proba(X_test_tfidf)[:,1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VWMH-WG7q7p"
      },
      "source": [
        "## Applying Support Vector Machines and Randomised Search\n",
        "\n",
        "** It is fine replace ** SVMs with random forests or other types of classifiers that you want to test out like KNN.\n",
        "\n",
        "To speed up the experiments here, you can start with just a [linear kernel](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html). Example of grid search for fine tuning RBF kernel parameter can be found [here](http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html#sphx-glr-auto-examples-svm-plot-rbf-parameters-py).\n",
        "And only use **the the first 1000 training data** for grid/randomised search.\n",
        "\n",
        "Note that: when peform grid search, the it's important to cover a broad enough range for some parameters (such as C and gamma for SVMs), oftne log space will be used instead of linear space: e.g. np.logspace(-3, -2, 3)\n",
        "\n",
        "### Question\n",
        "Do you need to normalise or scale the input data in this case?\n",
        "\n",
        "### Exercise\n",
        "Use [randomizedSearch](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) to optimise hyperparameters: C, and gamma for RBF kernels, or just C for linear kernels. You may compare the time used the performance and time used between the two methods.\n",
        "\n",
        "Better to start with a few values in the grid to test the code. And the grid can be refined further.\n",
        "\n",
        "**Note**: Randomised search is usually works much more efficiently (in computation) than the grid search.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "WHEIiP3w7q7p"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "# C_range = np.logspace(-3, -2, 3)\n",
        "# gamma_range = np.logspace(-9, 3, 3)\n",
        "# param_grid = {'kernel=('linear'), 'C'=[0.01,1,10])\n",
        "# Your code here: Note: you can use the 1st 1000 data for hyperparameter tuning in this exercise.\n",
        "param_grid = ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ...\n",
        "#print(\"The best parameters are %s with a score of %0.2f\"\n",
        "#      % (grid.best_params_, grid.best_score_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtFeBOrSBGmH"
      },
      "source": [
        "#### Question: How does the change of C and kernel parameter such as gamma for RBFs, affect the model performance?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXMJi6SO7q7p"
      },
      "outputs": [],
      "source": [
        "# Train and test the SVM classifier with the 'best' parameter chosen from CV\n",
        "# Hint: use all training example this time to train the model\n",
        "# Your code here\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "647735dc-9093-af47-528c-41a4034f8afb",
        "_uuid": "3b49c596d521b0bac72cbfb3c8508217104edb3e",
        "id": "ITgM1wZ17q7p"
      },
      "source": [
        "## Results\n",
        "\n",
        "In order to compare our learning algorithms, let's build the ROC curve. The curve with the highest AUC value will show our \"best\" algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fb05c625-efb2-3aa2-eb01-e74983620986",
        "_uuid": "92734eb9622cced1fb58c9f10f24fa64a56fbaf7",
        "id": "423_SdJ97q7q"
      },
      "outputs": [],
      "source": [
        "def formatt(x):\n",
        "    if x == 'negative':\n",
        "        return 0\n",
        "    return 1\n",
        "vfunc = np.vectorize(formatt)\n",
        "\n",
        "cmp = 0\n",
        "colors = ['b', 'g', 'y', 'm', 'k']\n",
        "for model, predicted in prediction.items():\n",
        "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predicted)\n",
        "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
        "    plt.plot(false_positive_rate, true_positive_rate, colors[cmp], label='%s: AUC %0.2f'% (model,roc_auc))\n",
        "    cmp += 1\n",
        "\n",
        "plt.title('Classifiers comparaison with ROC')\n",
        "plt.legend(loc='lower right')\n",
        "plt.plot([0,1],[0,1],'r--')\n",
        "plt.xlim([-0.1,1.2])\n",
        "plt.ylim([-0.1,1.2])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9f339d20-b863-9d25-2f6e-5c45f96bea6f",
        "_uuid": "086ce0446f152d15ff46234f0759f6b650795267",
        "id": "wWgeEdtN7q7q"
      },
      "source": [
        "### Question:\n",
        "\n",
        "Based on the ROC curves and AUC value, which method provides the best results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fc7d7fbc-7efe-b2f3-795e-d81bfea0da4a",
        "_uuid": "59a9cfae7ac7c62f49b6d60d7f1664dbb5fd4864",
        "id": "1tdy7GCG7q7q"
      },
      "source": [
        "Let's focus on logistic regression, and check the accuracy, recall and confusion matrix of this model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b69532d8-7841-ba8f-8091-34d13f69a880",
        "_uuid": "a144110d5bca0a99ee69f02de73f268ec11ce0cb",
        "id": "w_iWruGe7q7q"
      },
      "outputs": [],
      "source": [
        "print(metrics.classification_report(y_test, prediction['Logistic'], target_names = [\"positive\", \"negative\"]))\n",
        "\n",
        "# In case of errors, fix the code so that the classsifcation report can be created properly\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "610b8b65-3362-de15-ac1c-61aa2e90b693",
        "_uuid": "1d3cce4c25db9d409606c3ce76918cbd8d1a93aa",
        "id": "jOLlNP0f7q7q"
      },
      "source": [
        "Let's also have a look at what the best & words are by looking at the coefficients:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3031ddc6-54d6-f7cb-2572-f2f931652548",
        "_uuid": "72802dabc4dc385fcdee2d493f68fc1ddc337ab7",
        "id": "dTSs73Kj7q7r"
      },
      "outputs": [],
      "source": [
        "words = count_vect.get_feature_names_out()\n",
        "feature_coefs = pd.DataFrame(\n",
        "    data = list(zip(words, logreg.coef_[0])),\n",
        "    columns = ['feature', 'coef'])\n",
        "\n",
        "feature_coefs.sort_values(by='coef')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8ba52b09-0b99-d70c-4264-570f4a4e5f38",
        "_uuid": "511cf659de6bb131a2beb9dace8c72ce9c51604a",
        "id": "bXB938Bt7q7r"
      },
      "outputs": [],
      "source": [
        "def test_sample(model, sample):\n",
        "    sample_counts = count_vect.transform([sample])\n",
        "    sample_tfidf = tfidf_transformer.transform(sample_counts)\n",
        "    result = model.predict(sample_tfidf)[0]\n",
        "    prob = model.predict_proba(sample_tfidf)[0]\n",
        "    print(result)\n",
        "    print(\"Sample estimated as %s: negative prob %f, positive prob %f\" % (result, prob[0], prob[1]))\n",
        "\n",
        "test_sample(logreg, \"The food was delicious, it smelled great and the taste was awesome\")\n",
        "test_sample(logreg, \"The whole experience was horrible. The smell was so bad that it literally made me sick.\")\n",
        "test_sample(logreg, \"The food was ok, I guess. The smell wasn't very good, but the taste was ok.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "01d6cbb8-a12e-35b5-398b-a4a926516bb0",
        "_uuid": "e43f1dc7508dccd2398537bef62dc6afceea5838",
        "id": "uk8Q05j_7q7r"
      },
      "source": [
        "There are also some weird predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5d5d6579-2175-5201-f1a5-2c1f0b466e67",
        "_uuid": "41c07de14d979443b1193cbc463051bc82ad37cd",
        "id": "_wHaOYks7q7r"
      },
      "outputs": [],
      "source": [
        "test_sample(logreg, \"The smell reminded me of ammonia\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhYQyQuV7q7r"
      },
      "source": [
        "### Exercise (optional, perhaps more fun?)\n",
        "\n",
        "You can create a spam detector for youtube comments using similar methods. The dataset you can use is 1956 comments from 5 different YouTube videos, available [here](https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some helper code to read out the dataset here, e.g.\n",
        "# Read the data in a folder and set the first 4 youtube datasets as traing and the 5th as testing set.\n",
        "#\n",
        "# After unzipping the downloaded datasets to a folder \"YouTube-Spam-Collection-v1\" (consists of 5 csv files \"Youtube01-xxx.csv\" to \"Youtub05-xxx.csv\").\n",
        "#\n",
        "# Read the data\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "files = glob.glob(dir+\"/YouTube-Spam-Collection-v1/*\")\n",
        "\n",
        "files = sorted(files)[:-1]\n",
        "for f in files:\n",
        "    temp = pd.read_csv(f)\n",
        "    print(\"(file, shape):\", f, temp.shape)\n",
        "    train_df = pd.concat([train_df, temp], ignore_index=True)\n",
        "\n",
        "print(\"the shape of training files\", train_df.shape)\n",
        "\n",
        "# Pick required columns\n",
        "train_df = train_df[['CONTENT', 'CLASS']]\n",
        "\n",
        "# Set the last dataset for test\n",
        "test_df = pd.read_csv(dir + '/YouTube-Spam-Collection-v1/Youtube05-Shakira.csv')\n",
        "test_df = test_df[['CONTENT', 'CLASS']]"
      ],
      "metadata": {
        "id": "NY3kNEUCVFNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UX95pzDI7V_k"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "#\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}